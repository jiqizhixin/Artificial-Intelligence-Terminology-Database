# Letter L
[*Return*](https://github.com/SyncedAI00/Artificial-Intelligence-Terminology/blob/master/README.md)
索引编号|英文术语|中文翻译|常用缩写|来源&扩展|备注
---|---|---|---|---|---
AITD-00953|L-BFGS|L-BFGS||[1]||
AITD-00954|Label|标签/标记||[1]||
AITD-00955|Label Propagation|标记传播||[1]||
AITD-00956|Label Smoothing|标签平滑||[1]||
AITD-00957|Label Space|标记空间||[1]||
AITD-00958|Labeled|标注||[1]||
AITD-00959|Lagrange Dual Problem|拉格朗日对偶问题||[1]||
AITD-00960|Lagrange Duality|拉格朗日对偶性||[1]||
AITD-00961|Lagrange Function|拉格朗日函数||[1]||
AITD-00962|Lagrange Multiplier|拉格朗日乘子||[1]||
AITD-00963|Language Model|语言模型||[1]||
AITD-00964|Language Modeling|语言模型化||[1]||
AITD-00965|Laplace Distribution|Laplace分布||[1]||
AITD-00966|Laplace Smoothing|拉普拉斯平滑||[1]||
AITD-00967|Laplacian Correction|拉普拉斯修正||[1]||
AITD-00968|Large Learning Step|大学习步骤||[1]||
AITD-00969|Las Vegas Method|拉斯维加斯方法||[1]||
AITD-00970|Latent|潜在||[1]||
AITD-00971|Latent Dirichlet Allocation|潜在狄利克雷分配|LDA|[[1]](https://www.jiqizhixin.com/articles/2017-09-01-7)||
AITD-00972|Latent Layer|潜层||[1]||
AITD-00973|Latent Semantic Analysis|潜在语义分析|LSA|[1]||
AITD-00974|Latent Semantic Indexing|潜在语义索引|LSI|[1]||
AITD-00975|Latent Variable|潜变量/隐变量||[1]||
AITD-00976|Law of Large Numbers|大数定律||[1]||
AITD-00977|Layer|层||[1]||
AITD-00978|Layer Normalization|层规范化||[1]||
AITD-00979|Layer-Wise|逐层的||[1]||
AITD-00980|Layer-Wise Adaptive Rate Scaling|逐层适应率缩放|LARS|[1]||
AITD-00981|Layer-Wise Normalization|逐层规范化||[1]||
AITD-00982|Layer-Wise Pretraining|逐层预训练||[1]||
AITD-00983|Layer-Wise Training|逐层训练||[1]||
AITD-00984|Lazy Learning|懒惰学习||[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3)||
AITD-00985|Leaf Node|叶结点||[1]||
AITD-00986|Leaky Lelu Function|泄漏线性整流函数||[1]||
AITD-00987|Leaky Relu|泄漏修正线性单元/泄漏整流线性单元||[1]||
AITD-00988|Leaky Unit|渗漏单元||[1]||
AITD-00989|Learned|学成||[1]||
AITD-00990|Learned Approximate Inference|学习近似推断||[1]||
AITD-00991|Learner|学习器||[1]||
AITD-00992|Learning|学习||[1]||
AITD-00993|Learning Algorithm|学习算法||[1]||
AITD-00994|Learning By Analogy|类比学习||[1]||
AITD-00995|Learning Rate|学习率||[1]||
AITD-00996|Learning Rate Annealing|学习率退火||[1]||
AITD-00997|Learning Rate Decay|学习率衰减||[1]||
AITD-00998|Learning Rate Warmup|学习率预热||[1]||
AITD-00999|Learning To Learn|学习的学习||[1]||
AITD-01000|Learning Vector Quantization|学习向量量化|LVQ|[1]||
AITD-01001|Least General Generalization|最小一般泛化||[1]||
AITD-01002|Least Mean Squares|最小均方|LMS|[1]||
AITD-01003|Least Square Method|最小二乘法|LSM|[[1]](https://www.jiqizhixin.com/articles/2017-09-24-5)||
AITD-01004|Least Squares Regression Tree|最小二乘回归树||[1]||
AITD-01005|Leave-One-Out Cross Validation|留一交叉验证||[1]||
AITD-01006|Leave-One-Out|留一法|LOO|[1]||
AITD-01007|Lebesgue-Integrable|勒贝格可积||[1]||
AITD-01008|Left Eigenvector|左特征向量||[1]||
AITD-01009|Left Singular Vector|左奇异向量||[1]||
AITD-01010|Leibniz's Rule|莱布尼兹法则||[1]||
AITD-01011|Lifelong Learning|终身学习||[1]||
AITD-01012|Likelihood|似然||[1]||
AITD-01013|Line Search|线搜索||[1]||
AITD-01014|Linear Auto-Regressive Network|线性自回归网络||[1]||
AITD-01015|Linear Chain|线性链||[1]||
AITD-01016|Linear Chain Conditional Random Field|线性链条件随机场||[1]||
AITD-01017|Linear Classification Model|线性分类模型||[1]||
AITD-01018|Linear Classifier|线性分类器||[1]||
AITD-01019|Linear Combination|线性组合| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3)|数学|
AITD-01020|Linear Dependence|线性相关||[1]||
AITD-01021|Linear Discriminant Analysis|线性判别分析|LDA|[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3)|统计、机器学习|
AITD-01022|Linear Factor Model|线性因子模型||[1]||
AITD-01023|Linear Mapping|线性映射||[1]||
AITD-01024|Linear Model|线性模型|LR|[[1]](https://www.nature.com/articles/s41557-021-00716-z)[[2]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3)|统计、机器学习|
AITD-01025|Linear Programming|线性规划||[1]||
AITD-01026|Linear Regression|线性回归||[[1]](https://www.jiqizhixin.com/articles/2018-01-01)[[2]](https://www.jiqizhixin.com/articles/2017-11-17-5)[[3]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3)|统计、数学|
AITD-01027|Linear Scaling Rule|线性缩放规则||[1]||
AITD-01028|Linear Scan|线性扫描||[1]||
AITD-01029|Linear Space|线性空间||[1]||
AITD-01030|Linear Support Vector Machine|线性支持向量机||[1]||
AITD-01031|Linear Support Vector Machine In Linearly Separable Case|线性可分支持向量机||[1]||
AITD-01032|Linear Threshold Units|线性阈值单元||[1]||
AITD-01033|Linear Transformation|线性变换||[1]||
AITD-01034|Linearly Independent|线性无关||[1]||
AITD-01035|Linearly Separable|线性可分||[1]||
AITD-01036|Linearly Separable Data Set|线性可分数据集||[1]||
AITD-01037|Link Analysis|链接分析||[1]||
AITD-01038|Link Function|联系函数||[1]||
AITD-01039|Link Prediction|链接预测||[1]||
AITD-01040|Link Table|连接表||[1]||
AITD-01041|Linkage|连接||[1]||
AITD-01042|Linked Importance Sampling|链接重要采样||[1]||
AITD-01043|Lipschitz|Lipschitz||[1]||
AITD-01044|Lipschitz Constant|Lipschitz常数||[1]||
AITD-01045|Lipschitz Continuous|Lipschitz连续||[1]||
AITD-01046|Liquid State Machine|流体状态机||[1]||
AITD-01047|Local Conditional Probability Distribution|局部条件概率分布||[1]||
AITD-01048|Local Constancy Prior|局部不变性先验||[1]||
AITD-01049|Local Contrast Normalization|局部对比度规范化||[1]||
AITD-01050|Local Curvature|局部曲率||[1]||
AITD-01051|Local Descent|局部下降||[1]||
AITD-01052|Local Invariances|局部不变性||[1]||
AITD-01053|Local Kernel|局部核||[1]||
AITD-01054|Local Markov Property|局部马尔可夫性||[1]||
AITD-01055|Local Maxima|局部极大值||[1]||
AITD-01056|Local Maximum|局部极大点||[1]||
AITD-01057|Local Minima|局部极小||[1]||
AITD-01058|Local Minimizer|局部最小解||[1]||
AITD-01059|Local Minimum|局部极小||[1]||
AITD-01060|Local Representation|局部式表示/局部式表征||[1]||
AITD-01061|Local Response Normalization|局部响应规范化|LRN|[1]||
AITD-01062|Locally Linear Embedding|局部线性嵌入|LLE|[1]||
AITD-01063|Log Likelihood|对数似然函数||[1]||
AITD-01064|Log Linear Model|对数线性模型||[1]||
AITD-01065|Log-Likelihood|对数似然||[1]||
AITD-01066|Log-Likelihood Loss Function|对数似然损失函数||[1]||
AITD-01067|Log-Linear Regression|对数线性回归||[1]||
AITD-01068|Logarithmic Loss Function|对数损失函数||[1]||
AITD-01069|Logarithmic Scale|对数尺度||[1]||
AITD-01070|Logistic Distribution|对数几率分布||[1]||
AITD-01071|Logistic Function|对数几率函数||[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00398/978-1-78801-789-3)||
AITD-01072|Logistic Loss|对率损失||[1]||
AITD-01073|Logistic Regression|对数几率回归|LR|[[1]](https://www.jiqizhixin.com/articles/2017-11-23-6)[[2]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00372/978-1-78801-789-3)|统计、机器学习|
AITD-01074|Logistic Sigmoid|对数几率Sigmoid||[1]||
AITD-01075|Logit|对数几率||[1]||
AITD-01076|Long Short Term Memory|长短期记忆|LSTM|[[1]](https://www.jiqizhixin.com/articles/2017-12-18-6)[[2]](https://www.jiqizhixin.com/articles/2017-10-04-2)[[3]](https://www.jiqizhixin.com/articles/2017-09-29-7)[[4]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3)||
AITD-01077|Long Short-Term Memory Network|长短期记忆网络|LSTM|[1]||
AITD-01078|Long-Term Dependencies Problem|长程依赖问题||[1]||
AITD-01079|Long-Term Dependency|长期依赖||[1]||
AITD-01080|Long-Term Memory|长期记忆||[1]||
AITD-01081|Loop|环||[1]||
AITD-01082|Loopy Belief Propagation|环状信念传播|LBP|[1]||
AITD-01083|Loss|损失||[1]||
AITD-01084|Loss Function|损失函数||[[1]](https://www.jiqizhixin.com/articles/2018-01-03-4)[[2]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3)|机器学习|
AITD-01085|Low Rank Matrix Approximation|低秩矩阵近似||[1]||
AITD-01086|Lp Distance|Lp距离||[1]||
AITD-02331|L1 And L2 Regularization|L1与L2正则化| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3)| |
AITD-02332|Laboratory Level|实验室级别| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3)| |
AITD-02333|Language Processing|语言处理| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3)| |
AITD-02334|Laplacian Prior|拉普拉斯先验| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3)| |
AITD-02335|Large-Scale Data Storage|大规模数据存储| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3)| |
AITD-02336|Lasers|激光器| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3)| |
AITD-02337|Lasso Regression|拉索回归| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00206/978-1-83916-023-3)| |
AITD-02338|LBP|局部二值模式| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00340/978-1-78801-789-3)| |
AITD-02339|Least Absolute Shrinkage And Selection Operator|Lasso回归|LASSO|[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3)| |
AITD-02340|Least Square Support Vector Machine|最小二乘支持向量机|LSSVM|[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00311/978-1-78801-789-3)| |
AITD-02341|Ligand-Field|配位场| |[[1]](https://www.nature.com/articles/s41557-021-00716-z)| |
AITD-02342|Linear|线性的| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00037/978-1-78801-789-3)[[2]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00016/978-1-78801-789-3)|数学|
AITD-02343|Linear Dimension Reduction Methods|线性降维方法| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00450/978-1-78801-789-3)| |
AITD-02344|Linear Vibronic Coupling Model|线性振子耦合模型| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00076/978-1-78801-789-3)| |
AITD-02345|Local Recurrent|本地卷积| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3)| |
AITD-02346|Logic And Heuristics Applied To Synthetic Analysis|LHASA 程序|LHASA|[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00169/978-1-78801-789-3)| |
AITD-02347|Long-Range Prediction|长期预测| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3)| |
AITD-02348|Long-Range Prediction Models|长期预测模型| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00227/978-1-78801-789-3)| |
AITD-02349|Long-Term Planning|长期规划| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00001/978-1-78801-789-3)| |
AITD-02350|Long-Term Reward|长期回报| |[[1]](https://pubs.rsc.org/en/content/chapter/bk9781788017893-00136/978-1-78801-789-3)| |
